{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "## Team B, CA Beaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import ast\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, insert\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from config import username\n",
    "from config import password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\kate_\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# initialize connection with Chrome Driver\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our base URL\n",
    "base_url = \"https://www.californiabeaches.com/beaches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the regions off the main beach page\n",
    "browser.visit(base_url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "cali_soup = soup.find(id=\"regions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# initialize empty county URL list\n",
    "county_urls = []    \n",
    "county_url = \"\"\n",
    "\n",
    "# scrape lists of counties in each region\n",
    "region_soup = cali_soup.find_all(\"ul\")\n",
    "\n",
    "\n",
    "for region in region_soup:\n",
    "    county_soup = region.find_all(\"a\", href=True)\n",
    "\n",
    "\n",
    "    for county in county_soup:\n",
    "        if county:\n",
    "            # we found one, so pull out the URL\n",
    "            new_url = county[\"href\"]\n",
    "            \n",
    "            # only save one copy of each url\n",
    "            if new_url != county_url:\n",
    "                county_url = new_url\n",
    "                x = county_url.split(\"/\")\n",
    "                \n",
    "                # split out the region and county\n",
    "                # and clean out hyphens\n",
    "                region = x[-3]\n",
    "                cnty = x[-2]\n",
    "                cnty = cnty.replace(\"-\", \" \")\n",
    "                cnty = cnty.replace(\" county\", \"\")\n",
    "                county_urls.append([region.title(), cnty.title(), county_url])\n",
    "                \n",
    "\n",
    "#print(len(county_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "# initialize empty area list\n",
    "area_urls = []    \n",
    "area_url = \"\"\n",
    "\n",
    "# loop through county URLs\n",
    "for county in county_urls:\n",
    "\n",
    "    # scrape the county webpage\n",
    "    browser.visit(county[2])\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # scrape the list of beaches\n",
    "    county_soup = soup.find(id=\"beach-list\")\n",
    "    \n",
    "    # scrape the links\n",
    "    area_soup = county_soup.find_all(\"a\", href=True)\n",
    "    \n",
    "    for area in area_soup:\n",
    "        if area:\n",
    "            # we have one, so pull out the URL\n",
    "            new_url = area[\"href\"]\n",
    "\n",
    "            if new_url != area_url:\n",
    "                area_url = new_url\n",
    "                x = area_url.split(\"/\")\n",
    "                \n",
    "                # split out the area name\n",
    "                # and clean out hyphens\n",
    "                curr_area = x[-2]\n",
    "                curr_area = curr_area.replace(\"-\", \" \")\n",
    "                area_urls.append([county[0], county[1], curr_area.title(), area_url])\n",
    "                \n",
    "#print(area_urls)\n",
    "#print(len(area_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Southern', 'San Diego', 'Carlsbad', 'Carlsbad City Beach', 'https://www.californiabeaches.com/beach/carlsbad-city-beach/']\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "# initialize empty beach list\n",
    "beach_urls = []    \n",
    "beach_url = \"\"\n",
    "\n",
    "for area in area_urls:\n",
    "    \n",
    "    # scrape the area page\n",
    "    browser.visit(area[3])\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # scrape the list of beaches\n",
    "    area_soup = soup.find(id=\"beach-list\")\n",
    "    \n",
    "    # scrape the links\n",
    "    beach_soup = area_soup.find_all(\"a\", href=True)\n",
    "    \n",
    "    for beach in beach_soup:\n",
    "        if beach:\n",
    "\n",
    "            # we found one, so pull out the URL\n",
    "            new_url = beach[\"href\"]\n",
    "\n",
    "            if new_url != beach_url:\n",
    "                beach_url = new_url\n",
    "                x = beach_url.split(\"/\")\n",
    "                # split out the beach name and\n",
    "                # clean out the hyphens\n",
    "                curr_beach = x[-2]\n",
    "                curr_beach = curr_beach.replace(\"-\", \" \")\n",
    "                beach_urls.append([area[0], area[1], area[2], curr_beach.title(), beach_url])\n",
    "                \n",
    "#print(beach_urls[0])\n",
    "#print(len(beach_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing: https://www.californiabeaches.com/beach/la-jolla-strand-beach/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/finney-street-beach/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/la-selva-beach/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/coyote-point-beach/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/bodega-dunes/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/navarro-point-preserve/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/caspar-headlands-state-natural-reserve/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/caspar-headlands-state-beach/, 'NoneType' object has no attribute 'find_all'\n",
      "Error processing: https://www.californiabeaches.com/beach/pelican-bluffs-preserve/, 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "for beach in beach_urls:\n",
    "    \n",
    "    # append an empty dictionary to each beach list\n",
    "    # to hold beach info\n",
    "    beach.append({})\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # scrape each beach page\n",
    "        browser.visit(beach[4])\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # scrape the list of data\n",
    "        data_soup = soup.find(\"dl\")\n",
    "\n",
    "        # scrape the titles and data values\n",
    "        title_soup = data_soup.find_all(\"dt\")\n",
    "        value_soup = data_soup.find_all(\"dd\")\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        # save column titles and data\n",
    "        for title in title_soup:\n",
    "            \n",
    "            if title.text == \"Address\":\n",
    "                addr_str = str(value_soup[i])\n",
    "                addr_br = addr_str.split(\"<br/>\")\n",
    "        \n",
    "                addr1_br = addr_br[0].split(\">\")\n",
    "                beach[5][\"address\"] = addr1_br[-1]\n",
    "        \n",
    "                addr2_br = addr_br[1].split(\"<\")\n",
    "                \n",
    "                city_state = addr2_br[0].split()\n",
    "                \n",
    "                beach[5][\"zip\"] = city_state[-1]\n",
    "                beach[5][\"state\"] = city_state[-2] \n",
    "                beach[5][\"city\"] = city_state[0].replace(\",\", \"\")              \n",
    "                    \n",
    "                gmap = ast.literal_eval(value_soup[i].find(\"span\")[\"data-gmapping\"])                \n",
    "               \n",
    "                beach[5][\"latitude\"] = gmap[\"latlng\"][\"lat\"]\n",
    "                beach[5][\"longitude\"] = gmap[\"latlng\"][\"lng\"]\n",
    "                \n",
    "#                print(lat, lng)\n",
    "\n",
    "        \n",
    "            elif title.text == \"Owner\":\n",
    "                owner = value_soup[i].text\n",
    "                beach[5][\"owner\"] = owner\n",
    "            \n",
    "                if value_soup[i].a:\n",
    "                    owner_url = value_soup[i].a[\"href\"]\n",
    "                    beach[5][\"owner_url\"] = owner_url\n",
    "        \n",
    "            else:\n",
    "                mod_title = title.text.replace(\" \", \"_\").lower()\n",
    "                beach[5][mod_title] = value_soup[i].text\n",
    "\n",
    "                                     \n",
    "            i+=1\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing: {beach[4]}, {e}\")\n",
    "        beach[0] = \"Not scraped\"\n",
    "        \n",
    "#    print(beach)\n",
    "\n",
    "#print(title_list)    \n",
    "#print(beach_urls[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-17ea3b082307>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# connect to SQL database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"postgresql://{username}:{password}@ec2-54-87-34-201.compute-1.amazonaws.com:5432/ddh5sm9o0kv98b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# connect to SQL database\n",
    "engine = create_engine(f\"postgresql://{username}:{password}@ec2-54-87-34-201.compute-1.amazonaws.com:5432/ddh5sm9o0kv98b\")\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reflect an existing database into a new model\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create references to our tables\n",
    "Beaches = Base.classes.beaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a database session\n",
    "session = Session(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear database before dump\n",
    "session.query(Beaches).delete()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list of data titles\n",
    "title_list = [\"address\", \"city\", \"state\", \"zip\", \"latitude\", \"longitude\", \"park_name\", \"owner\", \"owner_url\", \"activities\", \"amenities\", \"pet_policy\", \"fees\", \"phone\", \"other_names\"]\n",
    "\n",
    "\n",
    "# loop through all the beaches we scraped\n",
    "for beach in beach_urls:\n",
    "    if beach[0] != \"Not scraped\":\n",
    "    \n",
    "        for title in title_list:\n",
    "            if title not in beach[5]:   \n",
    "                 beach[5][title] = \"\"\n",
    "                    \n",
    "        if \"No \" in beach[5][\"pet_policy\"] or \"not allowed\" in beach[5][\"pet_policy\"]:\n",
    "            # no pets allowed\n",
    "            pets_allowed = \"N\"\n",
    "        else:\n",
    "            pets_allowed = \"Y\"\n",
    "            \n",
    "        if \"free\" not in beach[5][\"fees\"] and \"Free\" not in beach[5][\"fees\"]:\n",
    "            # free parking somewhere near beach\n",
    "            free_parking = \"N\"\n",
    "        else:\n",
    "            free_parking = \"Y\"\n",
    "    \n",
    "        # add data to database\n",
    "        new_beach = Beaches(region = beach[0], county = beach[1], area = beach[2], beach_name = beach[3], \\\n",
    "                        beach_url = beach[4], address = beach[5][\"address\"], city = beach[5][\"city\"], \\\n",
    "                        state = beach[5][\"state\"], zip = beach[5][\"zip\"], latitude = beach[5][\"latitude\"], \\\n",
    "                        longitude = beach[5][\"longitude\"], park_name = beach[5][\"park_name\"], \\\n",
    "                        owner = beach[5][\"owner\"], owner_url = beach[5][\"owner_url\"], \\\n",
    "                        activities = beach[5][\"activities\"], amenities = beach[5][\"amenities\"], \\\n",
    "                        pet_policy = beach[5][\"pet_policy\"], pets_allowed = pets_allowed, fees = beach[5][\"fees\"], \\\n",
    "                        free_parking = free_parking, phone = beach[5][\"phone\"], other_names = beach[5][\"other_names\"])\n",
    "    \n",
    "\n",
    "        session.add(new_beach)\n",
    "        \n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
